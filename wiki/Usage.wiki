#summary How to use yadmt

== High level instructions: ==

 # If you are using cluster, 
   * Make sure that you have setup [http://www.thecloudavenue.com/2012/01/how-to-setup-password-less-ssh-to.html passwordless ssh] over your cluster.
   * Create a login file "~/yadmt/loginFile" containing the names of machine (For Amazon EC2, use private DNS). Use ':' for local machine
 # Create a file "~/yadmt/files.txt" that contains path of input files. The input file should be in format suggested by  [http://svmlight.joachims.org/ svmlight]. The below commands should create files.txt in the folder "~/yadmt" containing full-path of all the files inside "my_data_directory".
{{{
cd my_data_directory
ls -d -1 $PWD/*.* > ~/yadmt/files.txt
}}}
 # Now run the following command that will walk you through the process:
{{{
cd ~/yadmt
./yadmt --wizard
}}}

== Different setup: ==

While using yadmt for classification, you would be interested in one of the following questions:
 # For a given dataset, find the best classifier (in this section, the classifier refers to "classifier+parameter" combo, i.e. "svm_poly+degree2")
   * See section "How to find best classifier"
 # Given many datasets, find the best classifier on each datasets.
   * This is same as asking question 1 on every dataset, i.e. treating every dataset independent of each other. Good news is you don't have to run _yadmt_ for every single dataset. All you have to do is modify [http://code.google.com/p/yadmt/source/browse/trunk/src/getParameterName.sh getParameterName] file and add path of every dataset in files.txt and _yadmt_ will take care of the rest.
 # Given many datasets and just 1 classifier, compare the classifier's performance across the datasets.
   * Since not many researchers are interested in this question, this feature is hidden intentionally. Set "COMPARE_DATASETS=1" in [http://code.google.com/p/yadmt/source/browse/trunk/src/yadmt.sh yadmt] to enable this (Note: only for advanced users). 
   * This is useful for people that have developed feature selection models like [http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation LDA] or [http://www.cs.cmu.edu/~lafferty/pub/ctm.pdf CTM] and wants to compare their efficacy using classification. In this case, we suggest that you re-run your models to generate _n_ datasets and specify 1 for number of cycles in _yadmt_ and not vice-versa.
 # Given many datasets and many classifiers, find the best classifier overall.
   * This is a tricky question and _yadmt_ doesnot answer that directly (although it does that indirectly by answering questions 2 and 3). We suggest that you run appropriate statistical tests on accuracy file and make your own decision (See [http://code.google.com/p/yadmt/source/browse/trunk/src/StatisticalTests.R StatisticalTests.R] for reference).
 # Given a very large dataset (that will definitely make off-the-self classifiers run out of memory), find the best classifier.
   * As a performance hack, you can randomly partition the dataset into smaller datasets and ask question 2. However, a word of caution, some classifiers have weird property that they might work well on smaller datasets and might not work so well on larger dataset (or at least there is no guarantee that they will). We suggest using [http://mahout.apache.org/ Apache Mahout] or writing your own Hadoop-based or disk-based classifier.
 # Given a newly developed classifier ("control classifier"), test its performance across traditional classifiers.
   * Feature under development.
 
== How to find best classifier ==

== Known Issues ==
 # If the number of elements for a given class is too low as compared to others, it could be the case that for either test or training data, there would be no element from that class. In this case, the Weka classifiers (but not the SVMs) could fail.